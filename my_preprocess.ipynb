{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to rotate image\n",
    "def rotate_image(image, angle):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    height, width = image.shape[:2]\n",
    "    center = (width / 2, height / 2)\n",
    "    scale = 1.0\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "    return rotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da436a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Frames and save in different directories\n",
    "def extract_frames(k, directory):\n",
    "    import os\n",
    "    import re\n",
    "    import cv2\n",
    "    import json\n",
    "    import mediapipe as mp\n",
    "    from tqdm.notebook import tqdm \n",
    "    import pandas as pd\n",
    "\n",
    "    bad_sample = [121]\n",
    "    report = pd.DataFrame()\n",
    "    report_stat = []\n",
    "    content = json.load(open('final_train.json'))\n",
    "    prog_gloss = tqdm()\n",
    "    if os.path.exists(directory):\n",
    "        video_ls = os.listdir(directory)\n",
    "    else:\n",
    "        raise Exception(\"Directory does not exist.\")\n",
    "    \n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    progress = tqdm(total=k)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "        for i, entry in enumerate(content):\n",
    "            gloss = entry['gloss']\n",
    "            instances = entry['instances']\n",
    "            progress.update(1)\n",
    "            progress.set_description(f\"Working on Gloss {gloss}\")\n",
    "            if not i < k:\n",
    "                #Reaching The top-k\n",
    "                break\n",
    "            prog_gloss.reset(0)\n",
    "            prog_gloss.total = len(instances)\n",
    "            for j, inst in enumerate(instances):\n",
    "                prog_gloss.update(1)\n",
    "                video_id = inst[\"video_id\"]\n",
    "                prog_gloss.set_description(f\"Getting {video_id}\")\n",
    "                video_directory = f'videos/{video_id}.mp4'\n",
    "                sign_id = inst[\"signer_id\"]\n",
    "                split = inst[\"split\"] #Train, Test or Validation\n",
    "                variation_id = inst[\"variation_id\"]\n",
    "                bbox = inst['bbox']\n",
    "                target_directory = f'frames/{split}/{gloss}/{variation_id}'\n",
    "\n",
    "                # Create Directory If not exist\n",
    "                make_directory(target_directory)\n",
    "\n",
    "                # Skip Bad Samples\n",
    "                if sign_id in bad_sample:\n",
    "                    print(f\"Skip Signer {sign_id}\")\n",
    "                    continue\n",
    "\n",
    "                if os.path.exists(video_directory):\n",
    "                    cap = cv2.VideoCapture(video_directory)\n",
    "                    frame_counter = 0\n",
    "                    while True:\n",
    "                        # Read a frame from the video file\n",
    "                        ret, frame = cap.read()\n",
    "                        \n",
    "                        # If the frame was not retrieved, then we have reached the end of the video\n",
    "                        if not ret:\n",
    "                            break\n",
    "\n",
    "                        # Remove Trash Frames with no pose/ two hands\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        results = holistic.process(frame)\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "                        if results.pose_landmarks is None:\n",
    "                            continue\n",
    "                        \n",
    "                        if results.left_hand_landmarks is None and results.right_hand_landmarks is None:\n",
    "                            continue\n",
    "\n",
    "                        cv2.imwrite(os.path.join(target_directory, f'{video_id}_{str(frame_counter).zfill(4)}.jpg'), frame)\n",
    "                        frame_counter += 1\n",
    "                    # Release the video capture object\n",
    "                    cap.release()\n",
    "                    report_stat.append([gloss, variation_id, video_id, sign_id, split, frame_counter, bbox])\n",
    "    report = pd.DataFrame(report_stat, columns=[\"gloss\", \"variation\",\"video id\",\"signer id\", \"split\", \"number of frames\", \"Bounding Box\"])\n",
    "    report.to_csv(\"frames/frame_report.csv\", index = False)\n",
    "    return report\n",
    "    \n",
    "                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this function for the first time, or when k is changed\n",
    "report_df = extract_frames(100, \"videos\")\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf10e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Merge two report\n",
    "# report_df = pd.read_csv(\"frames/frame_report.csv\")\n",
    "# letter_df = pd.read_csv(\"frames/frame_report_new.csv\")\n",
    "# report_df = pd.concat([report_df, letter_df])\n",
    "# report_df.to_csv('frames/frame_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5700e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "report_df = pd.read_csv(\"frames/frame_report.csv\")\n",
    "display(report_df.groupby(['gloss','variation'])['number of frames'].count()>10)\n",
    "report_df.groupby(['gloss','variation']).filter(lambda x: len(x) >= 14).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230a58d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_empty_dirs(path):\n",
    "    import os\n",
    "    # list all directories and files\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # iterate over all directories\n",
    "        for name in dirs:\n",
    "            full_path = os.path.join(root, name)\n",
    "            # if directory is empty, remove it\n",
    "            if not os.listdir(full_path):\n",
    "                os.rmdir(full_path)\n",
    "                print(f\"Removed empty directory: {full_path}\")\n",
    "\n",
    "# Only keep Gloss, Variation with at least 10 samples in total (train, test, val)\n",
    "def remove_minor_samples(df, min_samples = 10):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    split_ls = ['train', 'test', 'val']\n",
    "    # remove empty samples\n",
    "    df = df[df['number of frames']!=0]\n",
    "    \n",
    "    df_remain = df.groupby(['gloss','variation']).filter(lambda x: len(x) >= min_samples)\n",
    "    to_remove = df.groupby(['gloss','variation']).filter(lambda x: len(x) < min_samples)\n",
    "    print(to_remove)\n",
    "    return\n",
    "    gloss_to_remove = to_remove['gloss'].unique()\n",
    "    for gloss in gloss_to_remove:\n",
    "        if len(gloss) == 1: # Skip letter\n",
    "            continue\n",
    "        variations_to_remove = to_remove[to_remove['gloss']==gloss].groupby('variation').filter(lambda x: len(x)<min_samples)['variation'].unique()\n",
    "        for variation in variations_to_remove:\n",
    "            for split in split_ls:\n",
    "                try:\n",
    "                    to_remove_dir = f'frames/{split}/{gloss}/{variation}'\n",
    "                    shutil.rmtree(to_remove_dir)\n",
    "                except:\n",
    "                    continue\n",
    "    remove_empty_dirs('frames')\n",
    "    df_remain = df_remain[['gloss','variation','video id','signer id','split','number of frames','Bounding Box']]\n",
    "    df_remain.to_csv(\"frames/frame_report.csv\", index = False)\n",
    "    return df_remain\n",
    "\n",
    "import pandas as pd\n",
    "report_df = pd.read_csv(\"frames/frame_report.csv\")\n",
    "remove_minor_samples(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_letter_frames(): # All non v2 versions are damaged\n",
    "    import os\n",
    "    import re\n",
    "    from tqdm.notebook import tqdm\n",
    "    frame_path = 'frames'\n",
    "    split_ls = ['train','test','val']\n",
    "    for split in split_ls:\n",
    "        regular_expression = r'^[a-zA-Z]_\\d{3}\\.jpg$'\n",
    "        path = os.path.join(frame_path, split)\n",
    "        for root, _, files in tqdm(os.walk(path)):\n",
    "            for file in files:\n",
    "                if 'test' in file or 'val' in file or bool(re.match(regular_expression, file)):\n",
    "                    print(file)\n",
    "                    os.remove(os.path.join(root, file))\n",
    "\n",
    "remove_letter_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "letters = 'a b c d e f g h i j k l m n o p q r s t u v w x y z'\n",
    "# All non v2 versions are damaged, Redistribute V2 among train/val/test\n",
    "def letter_redistribute(): \n",
    "    import os\n",
    "    import re\n",
    "    from tqdm.notebook import tqdm\n",
    "    regular_expression = r'^[a-zA-Z]_V2_\\d{3}\\.jpg$'\n",
    "    frame_path = 'frames'\n",
    "    split_ls = ['train','train','val']\n",
    "    letter_ls = letters.split(' ')\n",
    "    for letter in letter_ls:\n",
    "        path = f'frames/train/{letter}/0'\n",
    "        v2_frames = []\n",
    "        for i in os.listdir(path):\n",
    "            if 'V2' in i:\n",
    "                v2_frames.append(i)\n",
    "        frame_count = len(v2_frames)\n",
    "\n",
    "        distribution = [frame_count*0.3//1, frame_count*0.6//1, frame_count]\n",
    "        letter_upper = letter.upper()\n",
    "        distribution_name = [f'{letter_upper}',f'{letter_upper}_V2', f'{letter_upper}_val']\n",
    "        counter = 0\n",
    "\n",
    "        for new_name, dir_name, num_of_frames in zip(distribution_name, split_ls, distribution):\n",
    "            file_counter = 0\n",
    "            while counter < num_of_frames:\n",
    "                original_name = f'{letter_upper}_V2_{str(counter).zfill(4)}.jpg'\n",
    "                new_file_name = f'{new_name}_{str(file_counter).zfill(4)}.jpg'\n",
    "                original_file_path = os.path.join(path, original_name)\n",
    "                new_file_path = os.path.join(f'frames/{dir_name}/{letter}/0', new_file_name)\n",
    "                make_directory(f'frames/{dir_name}/{letter}/0')\n",
    "                try:\n",
    "                    os.rename(original_file_path, new_file_path)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "                counter += 1\n",
    "                file_counter += 1\n",
    "        \n",
    "\n",
    "\n",
    "# Causion: Do not run more than once!! result will not be expected\n",
    "letter_redistribute()\n",
    "\n",
    "# If accidently run twich, it should raise error in lteration of letter A\n",
    "# To recover, remove all files in train/test/val of A, put all files to train of A from backup of A.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c45f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_frames(source, dst, video_id):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Check if target directory exists, if not, create it\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "\n",
    "    # Iterate over all files in source directory\n",
    "    for filename in os.listdir(source):\n",
    "        # If file's name starts with the given prefix, move it to the target directory\n",
    "        if filename.startswith(str(video_id)):\n",
    "            shutil.move(os.path.join(source, filename), dst)\n",
    "\n",
    "# Redistribute Samples, So All Minor Samples at least have some data\n",
    "def redistribute_samples(df, at_least_amount = 2, reduce_least_amount = 6):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    gloss_ls = df['gloss'].unique()\n",
    "    split_ls = ['train', 'val', 'test']\n",
    "    if at_least_amount * 2 >= reduce_least_amount:\n",
    "        print(\"Error in parameters\")\n",
    "        return\n",
    "    for gloss in gloss_ls:\n",
    "        if len(gloss) == 1:\n",
    "            continue\n",
    "        variation_ls = df[df['gloss']==gloss]['variation'].unique()\n",
    "        for variation in variation_ls:\n",
    "            df_focus = df[(df['gloss']==gloss) & (df['variation']==variation)]\n",
    "            split_sample_count = [0, 0, 0]\n",
    "            largest_set = -1\n",
    "            largest_count = -1\n",
    "            # Find which split has largest sample size\n",
    "            for i, split in enumerate(split_ls):\n",
    "                df_split = df_focus[df_focus['split']==split]\n",
    "                split_count = len(df_split.index)\n",
    "                split_sample_count[i] = split_count\n",
    "                if split_count > largest_count:\n",
    "                    largest_set = i\n",
    "                    largest_count = split_count\n",
    "            # Probably this function is repeatly called\n",
    "            if largest_count <= reduce_least_amount:\n",
    "                continue\n",
    "            move_count = 0 # index of which video to move\n",
    "            video_id_to_move_ls = df_focus[df_focus['split']==split_ls[largest_set]]['video id'].to_list()\n",
    "            source_dir = f'frames/{split_ls[largest_set]}/{gloss}/{variation}'\n",
    "            for i, split in enumerate(split_ls):\n",
    "                if i == largest_set:\n",
    "                    continue\n",
    "                dst_dir = f'frames/{split}/{gloss}/{variation}'\n",
    "                num_videos_to_move = at_least_amount - split_sample_count[i]\n",
    "                for j in range(num_videos_to_move):\n",
    "                    video_ld_to_move = video_id_to_move_ls[move_count]\n",
    "                    df.iloc[df['video id']==video_ld_to_move, df.columns=='split'] = split # Update df\n",
    "                    move_frames(source_dir, dst_dir, video_ld_to_move)\n",
    "                    move_count+=1\n",
    "    df.to_csv(\"frames/frame_report.csv\", index = False)\n",
    "    \n",
    "import pandas as pd\n",
    "report_df = pd.read_csv(\"frames/frame_report.csv\")\n",
    "redistribute_samples(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271178e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define two hands frame > 50 % as two hands gesture\n",
    "def count_hands():\n",
    "    import json\n",
    "    import mediapipe as mp\n",
    "    import cv2\n",
    "    import os\n",
    "    from tqdm.notebook import tqdm\n",
    "    mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "    result_dict = {}\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        gloss_ls = os.listdir(f'frames/train')\n",
    "        for gloss in tqdm(gloss_ls):\n",
    "            variation_ls = os.listdir(f'frames/train/{gloss}')\n",
    "            gloss_dict = {}\n",
    "            for variation in variation_ls:\n",
    "                two_hands_count = 0\n",
    "                total_frame_count = 0\n",
    "                image_ls = os.listdir(f'frames/train/{gloss}/{variation}')\n",
    "                for image_path in image_ls:\n",
    "                    image = cv2.imread(f'frames/train/{gloss}/{variation}/{image_path}')\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    results = holistic.process(image)\n",
    "\n",
    "                    # Skip Empty Frames\n",
    "                    if results.right_hand_landmarks is None and results.left_hand_landmarks is None:\n",
    "                        continue\n",
    "                    if results.right_hand_landmarks is not None and results.left_hand_landmarks is not None:\n",
    "                        two_hands_count += 1\n",
    "                    total_frame_count += 1\n",
    "                print(f'Gloss: {gloss} Variation: {variation} Two Hands Count: {two_hands_count}, total_frame_count: {total_frame_count}')\n",
    "                gloss_dict[variation] = two_hands_count > (total_frame_count//2)\n",
    "                result_dict[gloss] = gloss_dict\n",
    "    with open('is_two_hands.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(result_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "count_hands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_reconstruct(split): #Reconstruct Video To speed up landmark fetching by reducing File I/O \n",
    "    import os\n",
    "    import cv2\n",
    "    from tqdm.notebook import tqdm\n",
    "    file_prog = tqdm()\n",
    "    split_path = os.path.join('frames',split)\n",
    "    for root, _, files in tqdm(os.walk(split_path)):\n",
    "        file_prog.reset()\n",
    "        file_prog.total = len(files)\n",
    "        frame_dirs = '\\\\'.join(root.split('\\\\')[1:])\n",
    "        video_path = os.path.join('videos_reconstruct',frame_dirs)\n",
    "        make_directory(video_path)\n",
    "\n",
    "        video_id = None\n",
    "        fourcc = None\n",
    "        out = None\n",
    "        for file in files:\n",
    "            if len(file.split(\"_\")) == 2:\n",
    "                video_id_current = file.split(\"_\")[0]\n",
    "            else:\n",
    "                video_id_current = \"_\".join(file.split(\"_\")[0:2])\n",
    "            frame = cv2.imread(os.path.join(root, file))\n",
    "            \n",
    "            if video_id_current != video_id:\n",
    "                video_id = video_id_current\n",
    "                # Define the codec using VideoWriter_fourcc and create a VideoWriter object\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "                out = cv2.VideoWriter(os.path.join(video_path,f'{video_id}.mp4'), fourcc, 30.0, (frame.shape[1], frame.shape[0]))\n",
    "                \n",
    "            out.write(frame)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "video_reconstruct('train')\n",
    "video_reconstruct('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fadb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_holistic():\n",
    "    import mediapipe as mp\n",
    "    import pandas as pd\n",
    "    import cv2\n",
    "    mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "    mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "        frame_num = 32\n",
    "        split = 'train'\n",
    "        variation = 0\n",
    "        test_path = f\"frames/train/all/0/01987_0000.jpg\"\n",
    "        print(test_path)\n",
    "        image = cv2.imread(test_path)\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        print(image.shape)\n",
    "        \n",
    "        #image = image[:, 209:495]\n",
    "        \n",
    "        height = 540\n",
    "        width_cut = int(height/1.33)\n",
    "        print(width_cut)\n",
    "        width = int(image.shape[1] * height/image.shape[0] * 1)\n",
    "        print(width)\n",
    "        image = cv2.resize(image, (width, height))\n",
    "        #image = cv2.copyMakeBorder(image,0,0,140,140,cv2.BORDER_CONSTANT)\n",
    "        image = cv2.resize(image, (int(image.shape[1]*1.5), height))\n",
    "#         \n",
    "#         image = cv2.resize(image,dsize=None,fx=0.8,fy=1.2,interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        #image = rotate_image(image, 45)\n",
    "        #image = cv2.rotate(image, cv2.ROTATE_45_CLOCKWISE)\n",
    "        \n",
    "        results = holistic.process(image)\n",
    "        print(dir(results.pose_landmarks))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        print(type(results.pose_landmarks.landmark[0]))\n",
    "        def standardize_landmarks(results):\n",
    "            if results is not None:\n",
    "                pivot = results.pose_landmarks.landmark[0]\n",
    "                scale = results.pose_landmarks.landmark[11].x - results.pose_landmarks.landmark[12].x\n",
    "                for i, pose in enumerate(results.pose_landmarks.landmark):\n",
    "                    print(f\"X{i}: \", (pose.x-pivot.x)/scale, \"Origin: \",(pose.x))\n",
    "                    print(f\"Y{i}: \", (pose.x-pivot.x)/scale, \"Origin: \",(pose.y))\n",
    "        \n",
    "        standardize_landmarks(results)\n",
    "        \n",
    "        cv2.imshow('Test Holistic', image)\n",
    "        cv2.waitKey(0)\n",
    "# Function to test if the frames really extracted\n",
    "\n",
    "test_holistic()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6010c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cebc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad7df378",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from mediapipe.framework.formats.landmark_pb2 import NormalizedLandmark\n",
    "\n",
    "class dataset:\n",
    "\n",
    "    content = json.load(open('final_train.json'))\n",
    "    is_two_hands_table = json.load(open('is_two_hands.json'))\n",
    "    empty_fill = np.array([0, 0, 0])\n",
    "    empty_pivot = NormalizedLandmark()\n",
    "    empty_pivot.x = 0\n",
    "    empty_pivot.y = 0\n",
    "    empty_pivot.z = 0\n",
    "    empty_pivot.visibility = 0\n",
    "\n",
    "    # keep_pose = [0,2,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "\n",
    "    def __init__(self, k, split, partition, name, test=False):\n",
    "        self.k = k\n",
    "        self.split = split\n",
    "        self.partition = partition\n",
    "        self.name = name\n",
    "        self.test = test\n",
    "        self.df = pd.read_csv('frames/frame_report.csv')\n",
    "\n",
    "    def get_variation(self, gloss_instances):\n",
    "        #Getting Variation of Gloss\n",
    "        max_variation = 0\n",
    "        for inst in gloss_instances:\n",
    "            max_variation = max(max_variation, inst['variation_id'])\n",
    "        return max_variation + 1 # Offset from 0\n",
    "\n",
    "    def _crop_image(self, image, midpt, max_min_diff, hands_in, ratio, long_scale, short_scale):\n",
    "        midpt = int(midpt)\n",
    "        shoudler_scale = 1 - (max_min_diff/2) # Keep Half Shoulder Spaces\n",
    "        height = image.shape[0]\n",
    "        # Ratio: Scaling base on resolution\n",
    "        # Long/ Short Scale: Scaling differ for hand majority\n",
    "        if hands_in == 0: # Left\n",
    "            width_length_half_left = int((height/ ratio) / shoudler_scale / 2 * long_scale)\n",
    "            width_length_half_right = int((height/ ratio) / shoudler_scale / 2 * short_scale)\n",
    "\n",
    "        elif hands_in == 1: # Right\n",
    "            width_length_half_left = int((height/ ratio) / shoudler_scale / 2 * short_scale)\n",
    "            width_length_half_right = int((height/ ratio) / shoudler_scale / 2 * long_scale)\n",
    "        elif hands_in == 2: # Two Hands\n",
    "            width_length_half_left = int((height/ ratio) / shoudler_scale / 2 * short_scale)\n",
    "            width_length_half_right = int((height/ ratio) / shoudler_scale / 2 * short_scale)\n",
    "\n",
    "        shift_left = max(int(midpt-width_length_half_left),0)\n",
    "        shift_right = min(int(midpt+width_length_half_right),image.shape[1])\n",
    "\n",
    "        image = image[:, shift_left:shift_right]\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _standard_scaling(self, image):\n",
    "        height= image.shape[0]\n",
    "        width = image.shape[1]\n",
    "        # Height > Weight\n",
    "        if image.shape[1] > image.shape[0]:\n",
    "            new_height = 540\n",
    "            new_width = int(width * new_height/height)\n",
    "        else:\n",
    "            new_width = 540\n",
    "            new_height = int(height * new_width/width)\n",
    "\n",
    "        if image.shape[0] != 0 and image.shape[1] != 0:\n",
    "            image = cv2.resize(image, (new_width, new_height))\n",
    "        return image\n",
    "\n",
    "    # Helper Function to rotate image\n",
    "    def _rotate_image(self, image, angle):\n",
    "        height, width = image.shape[:2]\n",
    "        center = (width / 2, height / 2)\n",
    "        scale = 1.0\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "        rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "        return rotated_image\n",
    "\n",
    "\n",
    "    def _get_midpoint(self, video_path, holistic):\n",
    "        # Find x mid point\n",
    "        if not os.path.exists(video_path):\n",
    "            return -1, None, None\n",
    "        image = cv2.imread(video_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        results = holistic.process(image)\n",
    "        min_x = 1\n",
    "        max_x = 0\n",
    "\n",
    "        if results.left_hand_landmarks is None and results.right_hand_landmarks is None:\n",
    "            return -1, None, None\n",
    "\n",
    "        if results.pose_landmarks is None:\n",
    "            return -1, None, None\n",
    "        else:\n",
    "            # A boundary in case left/ right hand is not detected\n",
    "            min_x = results.pose_landmarks.landmark[0].x\n",
    "            max_x = results.pose_landmarks.landmark[0].x\n",
    "\n",
    "\n",
    "        if results.left_hand_landmarks is not None:\n",
    "            for landmark in results.left_hand_landmarks.landmark:\n",
    "                min_x = min(landmark.x, min_x)\n",
    "                max_x = max(landmark.x, max_x)\n",
    "                hands_in = 0\n",
    "\n",
    "        if results.right_hand_landmarks is not None:\n",
    "            for landmark in results.right_hand_landmarks.landmark:\n",
    "                min_x = min(landmark.x, min_x)\n",
    "                max_x = max(landmark.x, max_x)\n",
    "                hands_in = 1\n",
    "\n",
    "        if results.left_hand_landmarks is not None and results.right_hand_landmarks is not None:\n",
    "            hasattrnds_in = 2\n",
    "\n",
    "        midpt=(min_x+max_x) /2 * image.shape[1]\n",
    "        max_min_diff = max_x-min_x\n",
    "        return hands_in, midpt, max_min_diff\n",
    "\n",
    "\n",
    "    # Fetch Landmarks with flipped/ rotated image, merge into a list with its status and result\n",
    "    # Further unpack in later code\n",
    "    def _get_landmarks_merge(self, image, holistic, midpt, hands_in, max_min_diff, visibility=False, scale=False,\n",
    "                             random_factors=[], flip_ls=None):\n",
    "        result = []\n",
    "        flip_ls = [] if flip_ls is None else flip_ls\n",
    "\n",
    "        image_backup = image.copy()\n",
    "        steps = random_factors['steps']\n",
    "\n",
    "        crop_midpt = random_factors.get('hw_ratio') is not None\n",
    "        border_random = random_factors.get('random_border_lr') is not None\n",
    "        scale_random = random_factors.get('scale_random') is not None\n",
    "        rotation_random = random_factors.get('rotation') is not None\n",
    "\n",
    "        if hands_in == -1 and crop_midpt:\n",
    "            return []\n",
    "\n",
    "        for i in range(-1, steps):\n",
    "            if i != -1: # Keep Original Image in first iteration\n",
    "                image = image_backup.copy() # Reset image change done by last iteration\n",
    "                if rotation_random:\n",
    "                    rotation_angle = random_factors['rotation'][i]\n",
    "                    image = self._rotate_image(image, rotation_angle)\n",
    "\n",
    "                if crop_midpt:\n",
    "                    hw_ratio = random_factors['hw_ratio'][i]\n",
    "                    midpt_shift = random_factors['midpt_shift'][i]\n",
    "                    short_scale = random_factors['width_scaling_short'][i]\n",
    "                    long_scale = random_factors['width_scaling_long'][i]\n",
    "                    image=self._crop_image(image, midpt=midpt*midpt_shift, max_min_diff=max_min_diff,hands_in=hands_in, ratio=hw_ratio, long_scale=long_scale, short_scale=short_scale)\n",
    "                if border_random:\n",
    "                    top = random_factors['random_border_tb'][0][i]\n",
    "                    bot = random_factors['random_border_tb'][1][i]\n",
    "                    left = random_factors['random_border_lr'][0][i]\n",
    "                    right = random_factors['random_border_lr'][1][i]\n",
    "                    image = cv2.copyMakeBorder(image,top,bot,left,right,cv2.BORDER_CONSTANT)\n",
    "\n",
    "                if scale_random:\n",
    "                    width_scale =  random_factors['scale_random'][0][i]\n",
    "                    height_scale =  random_factors['scale_random'][1][i]\n",
    "\n",
    "                    if image.shape[0] == 0 or image.shape[1] == 0:\n",
    "                        break\n",
    "                    image = cv2.resize(image,dsize=None,fx=width_scale,fy=height_scale,interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            image = self._standard_scaling(image)\n",
    "            for f in range(-1, len(flip_ls)): # -1 for origin landmark fetching\n",
    "                base_image = image.copy()\n",
    "                flip_image = base_image\n",
    "                if f != -1:\n",
    "                    flip_image = cv2.flip(base_image, flip_ls[f])\n",
    "#                 cv2.imshow('Test Holistic', flip_image)\n",
    "#                 cv2.waitKey(0)\n",
    "                status, landmark = self._get_flatten_landmarks(flip_image, holistic, visibility, scale)\n",
    "                dict_landmark = {'status': status, 'landmark':landmark}\n",
    "                result.append(dict_landmark)\n",
    "#                 for r in range(1, rotation_step):\n",
    "#                     angle = 360/ rotation_step * r\n",
    "#                     base_image_flip = flip_image.copy()\n",
    "#                     rotate_image = self._rotate_image(base_image_flip, angle)\n",
    "#                     status, landmark = self._get_flatten_landmarks(rotate_image, holistic, visibility, scale)\n",
    "#                     dict_landmark = {'status': status, 'landmark':landmark}\n",
    "#                     result.append(dict_landmark)\n",
    "        return result\n",
    "\n",
    "    def _distance(self, pt1, pt2):\n",
    "        return ((pt1.x-pt2.x)**2 + (pt1.y-pt2.y)**2 + (pt1.z-pt2.z)**2)**0.5\n",
    "\n",
    "    # Return: status, Landmarks, left vaild landmark, right valid landmark\n",
    "    # Valid landmark: landmarks that are detected, else empty list\n",
    "    # status: -1: not detected, 0: two hands, 1: left hand only, 2: right hand only\n",
    "    # TODO: Handle Rotation/ Flip values here later\n",
    "    def _get_flatten_landmarks(self, image, holistic, use_visibility=False, scale=False):\n",
    "        if image is None:\n",
    "            return -1, [None]\n",
    "        if(image.shape[0]==0 or image.shape[1]==0):\n",
    "            return -1, [None]\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        status = -1\n",
    "\n",
    "        if self.test:\n",
    "            mp_holistic = mp.solutions.holistic\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            # Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                     mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "\n",
    "            # Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                     mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "\n",
    "            # Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                     mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "            cv2.imshow(\"preview\",image)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        # Extract landmarks\n",
    "        pose = results.pose_landmarks\n",
    "        left_hand = results.left_hand_landmarks\n",
    "        right_hand = results.right_hand_landmarks\n",
    "\n",
    "        # Counting this to prevent the image show a tiny part of hands (20 is the max)\n",
    "        left_boundary_count = 0\n",
    "        right_boundary_count = 0\n",
    "\n",
    "        if pose is not None:\n",
    "            if scale:\n",
    "                pivot = pose.landmark[0]\n",
    "                a, b = pose.landmark[11], pose.landmark[12]\n",
    "                scale_length = self._distance(a,b)\n",
    "            else:\n",
    "                pivot = dataset.empty_pivot\n",
    "                scale_length = 1\n",
    "\n",
    "            pose_row = None\n",
    "            for i, landmark in enumerate(pose.landmark):\n",
    "#                 if i not in dataset.keep_pose:\n",
    "#                     continue # Skip Non-informative Landmarks\n",
    "                landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "                landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "                landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "                visibility = landmark.visibility\n",
    "\n",
    "                pose_row = landmark_x if pose_row is None else np.append(pose_row, landmark_x)\n",
    "                pose_row = np.append(pose_row, landmark_y)\n",
    "                pose_row = np.append(pose_row, landmark_z)\n",
    "                if use_visibility:\n",
    "                    pose_row = np.append(pose_row, visibility)\n",
    "        else:\n",
    "            status = -1\n",
    "            # Invalid Image\n",
    "            return status, [None]\n",
    "\n",
    "        if left_hand is not None:\n",
    "            if scale:\n",
    "                pivot_left = left_hand.landmark[0]\n",
    "                a, b = left_hand.landmark[5], left_hand.landmark[0]\n",
    "                scale_length_left = self._distance(a,b)\n",
    "                \n",
    "            left_row = None\n",
    "            for landmark in left_hand.landmark:\n",
    "                landmark_x_relative_hand = (landmark.x-pivot_left.x)/scale_length_left\n",
    "                landmark_y_relative_hand = (landmark.y-pivot_left.y)/scale_length_left\n",
    "                landmark_z_relative_hand = (landmark.z-pivot_left.z)/scale_length_left\n",
    "                \n",
    "                landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "                landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "                landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "\n",
    "                left_row = landmark_x if left_row is None else np.append(left_row, landmark_x)\n",
    "                left_row = np.append(left_row, landmark_y)\n",
    "                left_row = np.append(left_row, landmark_z)\n",
    "\n",
    "                left_row = np.append(left_row, landmark_x_relative_hand)\n",
    "                left_row = np.append(left_row, landmark_y_relative_hand)\n",
    "                left_row = np.append(left_row, landmark_z_relative_hand)\n",
    "\n",
    "                if (not (landmark.x > 0  and landmark.x < 1)) or not (landmark.y > 0  and landmark.y < 1):\n",
    "                    left_boundary_count+=1\n",
    "\n",
    "        if right_hand is not None:\n",
    "            if scale:\n",
    "                pivot_right = right_hand.landmark[0]\n",
    "                a, b = right_hand.landmark[5], right_hand.landmark[0]\n",
    "                scale_length_right = self._distance(a,b)\n",
    "\n",
    "            right_row = None\n",
    "            for landmark in right_hand.landmark:\n",
    "                landmark_x_relative_hand = (landmark.x-pivot_right.x)/scale_length_right\n",
    "                landmark_y_relative_hand = (landmark.y-pivot_right.y)/scale_length_right\n",
    "                landmark_z_relative_hand = (landmark.z-pivot_right.z)/scale_length_right\n",
    "                \n",
    "                landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "                landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "                landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "\n",
    "                right_row = landmark_x if right_row is None else np.append(right_row, landmark_x)\n",
    "                right_row = np.append(right_row, landmark_y)\n",
    "                right_row = np.append(right_row, landmark_z)\n",
    "                \n",
    "                right_row = np.append(right_row, landmark_x_relative_hand)\n",
    "                right_row = np.append(right_row, landmark_y_relative_hand)\n",
    "                right_row = np.append(right_row, landmark_z_relative_hand)\n",
    "                \n",
    "                if (not (landmark.x > 0  and landmark.x < 1)) or not (landmark.y > 0  and landmark.y < 1):\n",
    "                    right_boundary_count+=1\n",
    "\n",
    "        # Two Hands Detected, but could be only showing a very tiny part\n",
    "        if left_hand is not None and right_hand is not None:\n",
    "            result = np.concatenate([pose_row,left_row,right_row])\n",
    "            result = result.reshape((1,result.shape[0]))\n",
    "            #print('both',result.shape)\n",
    "            status = 0\n",
    "            if left_boundary_count > 10: # Left Hand Invalid\n",
    "                status = 2\n",
    "            if right_boundary_count > 10: # Right Hand Invalid\n",
    "                status = 1\n",
    "            if left_boundary_count > 10 and right_boundary_count > 10: # Two Hands Are Invalid\n",
    "                status = -1\n",
    "        else:\n",
    "            # x->0.5 = middle, y->1 = bottom\n",
    "\n",
    "            #Single Hand Model\n",
    "            if left_hand is not None:\n",
    "                empty_arr = np.tile(dataset.empty_fill, 21*2)\n",
    "                result = np.concatenate([pose_row,left_row,empty_arr])\n",
    "                result = result.reshape((1,result.shape[0]))\n",
    "                #print('left',result.shape)\n",
    "                status = 1\n",
    "            elif right_hand is not None:\n",
    "                empty_arr = np.tile(dataset.empty_fill, 21*2)\n",
    "                result = np.concatenate([pose_row, empty_arr, right_row])\n",
    "                result = result.reshape((1,result.shape[0]))\n",
    "                #print('right',result.shape)\n",
    "                status = 2\n",
    "            else:\n",
    "                result = [None]\n",
    "                status = -1\n",
    "        # print(result.shape)\n",
    "        if self.test:\n",
    "            print(result)\n",
    "\n",
    "        return status, result\n",
    "\n",
    "    def _save_npy(self, save_directory, gloss, dataset):\n",
    "        for j, partition in enumerate(dataset):\n",
    "            index_col = None\n",
    "            gloss_col = None\n",
    "            label_col = None\n",
    "            variation_col = None\n",
    "            for i, data_variation in enumerate(partition):\n",
    "                if data_variation is None:\n",
    "                    print(f\"No data for {gloss}, partition:{j}\")\n",
    "                    continue\n",
    "                for row_idx in range(data_variation.shape[0]):\n",
    "                    gloss_directory = f'np_arrays/{save_directory}/{self.split}_{j}/{gloss}'\n",
    "                    row = data_variation[row_idx]\n",
    "                    # Create Directory If not exist\n",
    "                    self._make_directory(gloss_directory)\n",
    "\n",
    "                    to_save_path = f'np_arrays/{save_directory}/{self.split}_{j}/{gloss}/{i}_{row_idx}.npy'\n",
    "                    with open(to_save_path, 'wb') as f:\n",
    "                        np.save(to_save_path, row)\n",
    "\n",
    "                label_name = f'{gloss}_{i}'\n",
    "                variation_full = np.full(data_variation.shape[0], i)\n",
    "                index = np.arange(data_variation.shape[0])\n",
    "                gloss_full = np.full(data_variation.shape[0], gloss)\n",
    "                label = np.full(data_variation.shape[0], label_name)\n",
    "\n",
    "                index_col = index if index_col is None else np.append(index_col, index)\n",
    "                gloss_col = gloss_full if gloss_col is None else np.append(gloss_col, gloss_full)\n",
    "                variation_col = variation_full if variation_col is None else np.append(variation_col, variation_full)\n",
    "                label_col = label if label_col is None else np.append(label_col, label)\n",
    "            if index_col is not None:\n",
    "                result = pd.DataFrame()\n",
    "                result['Index'] = index_col\n",
    "                result['Gloss'] = gloss_col\n",
    "                result['Variation'] = variation_col\n",
    "                result['Label'] = label_col\n",
    "                result.to_csv(f'np_arrays/{save_directory}/{self.split}_{j}/{gloss}/meta.csv',index=False)\n",
    "\n",
    "\n",
    "    def _rng_upsampling(self, sample_set, window, max_duplicate=4):\n",
    "        result = []\n",
    "        remain = sample_set\n",
    "        shape_remain = sample_set.shape[0]\n",
    "        for i in range(sample_set.shape[0]):\n",
    "            target_row = sample_set[i]\n",
    "            # Window - (Shape remain) - Current Index\n",
    "            # Shape remain = sample shape - (i + 1)\n",
    "            shape_remain = sample_set.shape[0]-i\n",
    "            random_range = min(window - shape_remain - len(result), max_duplicate) + 1 # Range of random value could be\n",
    "            random_duplicate = np.random.randint(0, random_range + 1) # The random value to determine how much a sample be duplicated\n",
    "            for j in range(random_duplicate):\n",
    "                result.append(target_row)\n",
    "                if len(result)+shape_remain == window:\n",
    "                    break\n",
    "            if len(result)+shape_remain == window:\n",
    "                break\n",
    "        result = np.array(result)\n",
    "        result = np.vstack([result, remain[i:]])\n",
    "        if result.shape[0] != window: # Fill last row if still some empty space\n",
    "            to_fill = np.full((window-result.shape[0], result.shape[1]), result[-1])\n",
    "            result = np.vstack([result, to_fill])\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _extract_lstm(self, sample_set, window=20, upsampling_times=5):\n",
    "        # Shape 0: How many samples\n",
    "        # Shape 1: How many Landmarks\n",
    "        # Output: Extra shape 2: Time of Landmark\n",
    "        result = []\n",
    "        if len(sample_set.shape) != 2:\n",
    "            # Empty sample set\n",
    "            return np.array(result)\n",
    "        if sample_set.shape[0] < 10:\n",
    "            # Not Enough Samples\n",
    "            return np.array(result)\n",
    "\n",
    "        if sample_set.shape[0] < window :\n",
    "            upsampling_times = upsampling_times * 3 # Upsample more for minor samples\n",
    "            upsampling_time = 1 if upsampling_times == 0 else upsampling_times# At least resample once to prevent lost of data\n",
    "            for i in range(upsampling_times):\n",
    "                rng_samp = self._rng_upsampling(sample_set, window)\n",
    "                result.append(rng_samp)\n",
    "            return np.array(result)\n",
    "\n",
    "        for i in range(sample_set.shape[0]-window):\n",
    "            temp_result = sample_set[i:i+window]\n",
    "            temp_result = np.array(temp_result)\n",
    "            result.append(temp_result)\n",
    "            for i in range(upsampling_times):\n",
    "                rng_samp = self._rng_upsampling(sample_set[i:i+(window//2)], window)\n",
    "                result.append(rng_samp)\n",
    "        result = np.array(result)\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "    def _make_directory(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def fetch_data(self, progress=0, window=20, rotation=False, visibility=False, scale=False, keep_noise_frame=False, flip=None, flatten=False, random_augmentation_steps=0, upsampling_times=10,midpt_crop=False,random_boarder=False):\n",
    "        flip = [] if flip is None else flip\n",
    "        k = self.k\n",
    "        gloss_pbar = tqdm(total=self.k)\n",
    "        pbar = tqdm()\n",
    "        labels_to_fetch = self.df[\"gloss\"].to_list()\n",
    "        for i, entry in enumerate(dataset.content):\n",
    "            gloss_pbar.update(1)\n",
    "            gloss = entry['gloss']\n",
    "            if i < progress:\n",
    "                continue # Skip to current progress\n",
    "            if not i < k:\n",
    "                #Reaching The top-k\n",
    "                break\n",
    "            gloss = entry['gloss']\n",
    "            if gloss not in labels_to_fetch:\n",
    "                continue\n",
    "            \n",
    "            title = f\"Gloss {i+1} {gloss}, {i}/{k}\"\n",
    "            gloss_pbar.set_description(title)\n",
    "            instances = entry['instances']\n",
    "            variations = self.get_variation(instances)\n",
    "            video_count = [0] * variations # Count Video for different variation\n",
    "            result = [[None] * variations for i in range(self.partition)] # Also Spaces for partitions\n",
    "            mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "                # Per Video of Gloss\n",
    "                pbar.total = len(instances)\n",
    "                pbar.refresh()\n",
    "                for inst in instances:\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description('Fetching Videos')\n",
    "                    split = inst[\"split\"] #Train, Test or Validation\n",
    "                    if split != self.split: # Skip other split\n",
    "                        continue\n",
    "\n",
    "                    hands_in = -1\n",
    "                    midpt = None\n",
    "                    max_min_diff = None # To Scale By Shoudler Diff\n",
    "                    video_id = inst[\"video_id\"]\n",
    "                    variation_id = inst[\"variation_id\"]\n",
    "                    frame_count = 0\n",
    "                    temp_data = {}\n",
    "                    to_partition = video_count[variation_id] % self.partition\n",
    "\n",
    "                    # For two hands indicating\n",
    "                    two_hands_count = 0\n",
    "                    total_frame_count = 0\n",
    "                    is_two_hands_gesture = dataset.is_two_hands_table.get(gloss,{}).get(str(variation_id))\n",
    "                    # Removed Label\n",
    "                    if is_two_hands_gesture is None:\n",
    "                        continue\n",
    "\n",
    "                    random_factors = {}\n",
    "                    random_factors['steps'] = random_augmentation_steps\n",
    "                    if midpt_crop:\n",
    "                        random_factors['hw_ratio'] = np.around(np.random.uniform(1.25,1.75,random_augmentation_steps),2)\n",
    "                        random_factors['midpt_shift'] = np.around(np.random.uniform(0.9,1.1,random_augmentation_steps),2)\n",
    "                        random_factors['width_scaling_long'] = np.around(np.random.uniform(1.2,1.4,random_augmentation_steps),2)\n",
    "                        random_factors['width_scaling_short'] = np.around(np.random.uniform(1.1,1.2,random_augmentation_steps),2)\n",
    "                        random_factors['scale_random'] = [np.around(np.random.uniform(0.8,1.2,random_augmentation_steps),2) for i in range(2)]\n",
    "                    if random_boarder:\n",
    "                        random_factors['random_border_lr'] = [np.random.randint(0,120,random_augmentation_steps) for i in range(2)]\n",
    "                        random_factors['random_border_tb'] = [np.random.randint(0,40,random_augmentation_steps) for i in range(2)]\n",
    "                    if rotation:\n",
    "                        random_factors['rotation'] = np.around(np.random.uniform(-20,20,random_augmentation_steps),2)\n",
    "\n",
    "                    video_path = f'videos_reconstruct/{split}/{gloss}/{variation_id}/{video_id}.mp4'\n",
    "\n",
    "                    if not os.path.exists(video_path):\n",
    "                        frame_count = -1 # Leave Loop Instantly\n",
    "                    else:\n",
    "                        cap = cv2.VideoCapture(video_path)\n",
    "                    # Per Frame of Video\n",
    "                    while frame_count < 300:\n",
    "                        if frame_count == -1:\n",
    "                            break\n",
    "                        if midpt is None and midpt_crop:\n",
    "                            # Smoothing, prevent determine by only one frame\n",
    "                            midpt_sum = 0\n",
    "                            max_min_diff_ls = []\n",
    "                            hands_in_ls = []\n",
    "                            valid_count = 0\n",
    "                            for i in range(5):\n",
    "                              # Not hard code 15 prevent 15 is none\n",
    "                              video_path_15shift = f'frames/{split}/{gloss}/{variation_id}/{video_id}_{str(frame_count+15+i).zfill(4)}.jpg'\n",
    "                              hands_in_temp, midpt_temp, max_min_diff_temp = self._get_midpoint(video_path_15shift, holistic)\n",
    "                              if hands_in_temp != -1:\n",
    "                                valid_count += 1\n",
    "                                midpt_sum+=midpt_temp\n",
    "                                max_min_diff_ls.append(max_min_diff_temp)\n",
    "                                hands_in_ls.append(hands_in_temp)\n",
    "                            if valid_count != 0:\n",
    "                                midpt = midpt_sum/ valid_count\n",
    "                                max_min_diff=max(max_min_diff_ls)\n",
    "                                hands_in = max(hands_in_ls, key= hands_in_ls.count)\n",
    "\n",
    "                        if midpt is None and midpt_crop:\n",
    "                            # Prevent midpoint finding failed in first iteration\n",
    "                            # print(\"Mid point error\")\n",
    "                            frame_count+=1\n",
    "                            continue\n",
    "\n",
    "                        ret, image = cap.read()\n",
    "                        if image is None:\n",
    "                            break\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                        frame_count += 1\n",
    "                        landmark_results = self._get_landmarks_merge(image, holistic, visibility=visibility, scale=scale, random_factors=random_factors, midpt=midpt, max_min_diff=max_min_diff, hands_in=hands_in, flip_ls=flip)\n",
    "\n",
    "                        for i, landmark_result in enumerate(landmark_results):\n",
    "                            status = landmark_result['status']\n",
    "                            landmark = landmark_result['landmark']\n",
    "                            if status == -1: # Error\n",
    "                                continue\n",
    "\n",
    "                            total_frame_count += 1\n",
    "                            if is_two_hands_gesture: #Only add two hands\n",
    "                                if status == 0:\n",
    "                                    two_hands_count += 1\n",
    "                                    temp_data[i] = landmark if i not in temp_data else np.vstack([temp_data[i], landmark])\n",
    "                                else:\n",
    "                                    if keep_noise_frame:\n",
    "                                        temp_data[i] = landmark if i not in temp_data else np.vstack([temp_data[i], landmark])\n",
    "                                    continue\n",
    "                            else:\n",
    "                                if status == 0: #Only add one hand\n",
    "                                    two_hands_count += 1\n",
    "                                    if keep_noise_frame:\n",
    "                                        temp_data[i] = landmark if i not in temp_data else np.vstack([temp_data[i], landmark])\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    temp_data[i] = landmark if i not in temp_data else np.vstack([temp_data[i], landmark])\n",
    "                    # End loop through a video here\n",
    "                    if frame_count != -1:\n",
    "                        cap.release()\n",
    "                    pbar.set_description(f\"{title}\")\n",
    "                    temp_result = None\n",
    "                    # Unexpected HandCount for each video\n",
    "                    if is_two_hands_gesture:\n",
    "                        if two_hands_count < (total_frame_count * 0.7):\n",
    "                            #print(f\"Two hands frame: {two_hands_count}\")\n",
    "                            pbar.set_description(f\"{title}\\tSkip: Unexpected two hand count\\n\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        if two_hands_count > (total_frame_count * 0.7):\n",
    "                            pbar.set_description(f\"{title}\\tSkip: Unexpected two hand count\\n\")\n",
    "                            continue\n",
    "\n",
    "                    if total_frame_count == 0: # File not exist\n",
    "                        pbar.set_description(f\"{title}\\tSkip: File Not Exist\\n\")\n",
    "                        continue\n",
    "\n",
    "                    # Merge different transformations and to LSTM\n",
    "                    for key in temp_data:\n",
    "                        if not flatten:\n",
    "                            result_data = self._extract_lstm(temp_data[key], window=window, upsampling_times=upsampling_times)\n",
    "                            check_non_empty = len(result_data.shape) == 3\n",
    "                        else:\n",
    "                            result_data = temp_data[key]\n",
    "                            check_non_empty = len(result_data.shape) == 2\n",
    "                        if check_non_empty: # Prevent Empty Data\n",
    "                            temp_result = result_data if temp_result is None else np.vstack([temp_result, result_data])\n",
    "                    # Not Enough Data Remain\n",
    "                    if temp_result is not None:\n",
    "                        check_empty_result = len(temp_result.shape) != 3 if not flatten else len(temp_result.shape) != 2\n",
    "                    if temp_result is None or check_empty_result:\n",
    "                        pbar.set_description(f\"{title}\\tSkip: Not Enough Data Remain\\n\")\n",
    "                        continue\n",
    "\n",
    "                    result[to_partition][variation_id] = temp_result if result[to_partition][variation_id] is None and not temp_result.shape[1]==0 \\\n",
    "                                                        else np.vstack([result[to_partition][variation_id], temp_result])\n",
    "                    video_count[variation_id] += 1 # For Partitioning\n",
    "                # Endloop of per Gloss Here\n",
    "            pbar.set_description(f'Saving Files...')\n",
    "            self._save_npy(self.name, gloss, result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691c4d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb19aca93ba94bf893d3b44f3f123920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602e667dcbf941eb985cccec04133d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7475b8162cf241f6996051883e0f0944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0680bb9807234d62bf72460c42df4301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_name = 'dataset_standardize(refine2)_boarder_rotation_20'\n",
    "num_gloss = 100 # If over max gloss, get max gloss, include the letters\n",
    "progress = 0\n",
    "flatten=False\n",
    "flip = [1]\n",
    "window = 20\n",
    "random_augmentation_steps=10\n",
    "upsampling_times=0\n",
    "rotation = True\n",
    "visibility=False\n",
    "scale=True\n",
    "midpt_crop=False\n",
    "keep_noise_frame=True\n",
    "random_boarder=True\n",
    "\n",
    "train_ds = dataset(num_gloss,'train',1,dir_name, test=False)\n",
    "train_ds.fetch_data(window=window,\n",
    "                    progress=progress,\n",
    "                    visibility=visibility,\n",
    "                    scale=scale,\n",
    "                    rotation=rotation,\n",
    "                    flip=flip, flatten=flatten,\n",
    "                    random_augmentation_steps=random_augmentation_steps,\n",
    "                    keep_noise_frame=keep_noise_frame,\n",
    "                    upsampling_times=upsampling_times,\n",
    "                    midpt_crop=midpt_crop,\n",
    "                    random_boarder=random_boarder)\n",
    "\n",
    "# test_ds = dataset(num_gloss,'test',1,dir_name)\n",
    "# test_ds.fetch_data(window=window, progress=progress, rotation_step=rotation_step, flip=flip, flatten=flatten, random_augmentation_steps=random_augmentation_steps, keep_noise_frame=keep_noise_frame,upsampling_times=upsampling_times)\n",
    "\n",
    "val_ds = dataset(num_gloss,'val',1,dir_name)\n",
    "val_ds.fetch_data(window=window,\n",
    "                    progress=progress,\n",
    "                    visibility=visibility,\n",
    "                    scale=scale,\n",
    "                    rotation=rotation,\n",
    "                    flip=flip, flatten=flatten,\n",
    "                    random_augmentation_steps=random_augmentation_steps,\n",
    "                    keep_noise_frame=keep_noise_frame,\n",
    "                    upsampling_times=upsampling_times,\n",
    "                    midpt_crop=midpt_crop,\n",
    "                    random_boarder=random_boarder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f34cec6a",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117f0dff599e4db6bea2deff2fd1f930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2f481a36e64c778d656a18b0803738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "      Gloss  Index\n",
      "0     yes_0      0\n",
      "1     yes_0      1\n",
      "2     yes_0      2\n",
      "3     yes_0      3\n",
      "4     yes_0      4\n",
      "...     ...    ...\n",
      "2677  yes_0   2677\n",
      "2678  yes_0   2678\n",
      "2679  yes_0   2679\n",
      "2680  yes_0   2680\n",
      "2681  yes_0   2681\n",
      "\n",
      "[2682 rows x 2 columns]\n",
      "Getting Scaler\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869149ac22604dbe847f56aa4353be6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687db6b832b47d6b09ced7e3752f97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "      Gloss  Index\n",
      "0     yes_0      0\n",
      "1     yes_0      1\n",
      "2     yes_0      2\n",
      "3     yes_0      3\n",
      "4     yes_0      4\n",
      "...     ...    ...\n",
      "1824  yes_0   1824\n",
      "1825  yes_0   1825\n",
      "1826  yes_0   1826\n",
      "1827  yes_0   1827\n",
      "1828  yes_0   1828\n",
      "\n",
      "[1829 rows x 2 columns]\n",
      "Getting Scaler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def make_directory(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "# Integrate all meta df\n",
    "# Get Scaler and Save\n",
    "def integrate(dir_path, split):\n",
    "    merge_df = pd.DataFrame()\n",
    "    split_path = os.path.join(dir_path, split)\n",
    "    npy_stack = {}\n",
    "    full_set = None\n",
    "    gloss_col = None\n",
    "    index_col = None\n",
    "    file_prog = tqdm()\n",
    "    flatten_set = []\n",
    "    # Divide h5 to multiple files (in same directory, by gloss)\n",
    "    # Calculate Mean, Variation\n",
    "    h5_dir = os.path.join(dir_path, 'h5',split)\n",
    "    make_directory(h5_dir)\n",
    "    \n",
    "    for root, _, files in tqdm(os.walk(split_path)):\n",
    "        file_prog.reset()\n",
    "        file_prog.total = len(files)\n",
    "        for file in files:\n",
    "            file_prog.update(1)\n",
    "            if f'.npy' in file:\n",
    "                variation = file.split('_')[0]\n",
    "                row = np.load(os.path.join(root, file))\n",
    "                if npy_stack.get(variation) is None:\n",
    "                    npy_stack[variation] = [np.expand_dims(row, axis=0)]\n",
    "                else:\n",
    "                    npy_stack[variation].append(np.expand_dims(row, axis=0))\n",
    "                \n",
    "        if len(npy_stack) != 0:\n",
    "            for variation in npy_stack:\n",
    "                # Construct np array\n",
    "                gloss_set = np.vstack(npy_stack[variation])\n",
    "                \n",
    "                # Get Flatten Set\n",
    "                flat = np.unique(gloss_set.reshape(-1, gloss_set.shape[-1]), axis=0)\n",
    "                flatten_set.append(flat)\n",
    "                \n",
    "                # Generate Meta\n",
    "                gloss = root.split(\"\\\\\")[-1]\n",
    "                label_name = f'{gloss}_{variation}'\n",
    "                gloss_col_temp = np.full(gloss_set.shape[0], label_name)\n",
    "                print(gloss)\n",
    "                \n",
    "                gloss_col = gloss_col_temp if gloss_col is None else np.concatenate([gloss_col, gloss_col_temp])\n",
    "                index_col = np.arange(gloss_set.shape[0]) if index_col is None else np.concatenate([index_col, np.arange(gloss_set.shape[0])])\n",
    "\n",
    "                gloss_col_temp = np.string_(gloss_col_temp)\n",
    "                # print(gloss_col_temp.decode('utf-8'))\n",
    "                with h5py.File(os.path.join(h5_dir,f\"{label_name}.h5\"), \"w\") as out:\n",
    "                  out.create_dataset(f'data', data=gloss_set)\n",
    "                  out.create_dataset(f'label',data=gloss_col_temp)\n",
    "            npy_stack.clear()\n",
    "\n",
    "                \n",
    "            \n",
    "    # merge_df = merge_df[['Index','Gloss','Variation','Label']]\n",
    "    merge_df['Gloss'] = gloss_col\n",
    "    merge_df['Index'] = index_col\n",
    "    print(merge_df)\n",
    "#     merge_df.to_csv(os.path.join(h5_dir, f'meta.csv'))\n",
    "\n",
    "    print(\"Getting Scaler\")\n",
    "    full_set = np.vstack(flatten_set)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(full_set)\n",
    "    with open(os.path.join(dir_path,f'{split}_scaler.pkl'), 'wb') as to_write:\n",
    "        pickle.dump(scaler, to_write)\n",
    "        \n",
    "path = 'np_arrays/dataset_standardize(refine2)_boarder_rotation_20'\n",
    "integrate(path, 'train_0')\n",
    "# integrate(path, 'train_1')\n",
    "integrate(path, 'val_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "label0 = pd.read_csv(os.path.join(path, 'h5',\"train_0\", 'meta.csv'))['Gloss'].unique()\n",
    "label1 = pd.read_csv(os.path.join(path, 'h5',\"val_0\", 'meta.csv'))['Gloss'].unique()\n",
    "\n",
    "print(\"label 1 missing\")\n",
    "for i in label0:\n",
    "    if i not in label1:\n",
    "        print(i)\n",
    "        \n",
    "print(\"label 0 missing\")\n",
    "for i in label1:\n",
    "    if i not in label0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7146cbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gloss</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389799</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>2677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389800</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389801</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>2679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389802</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389803</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389804 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Gloss  Index\n",
       "0         a_0      0\n",
       "1         a_0      1\n",
       "2         a_0      2\n",
       "3         a_0      3\n",
       "4         a_0      4\n",
       "...       ...    ...\n",
       "389799  yes_0   2677\n",
       "389800  yes_0   2678\n",
       "389801  yes_0   2679\n",
       "389802  yes_0   2680\n",
       "389803  yes_0   2681\n",
       "\n",
       "[389804 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gloss</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149699</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149700</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>1825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149701</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>1826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149702</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149703</th>\n",
       "      <td>yes_0</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149704 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Gloss  Index\n",
       "0         a_0      0\n",
       "1         a_0      1\n",
       "2         a_0      2\n",
       "3         a_0      3\n",
       "4         a_0      4\n",
       "...       ...    ...\n",
       "149699  yes_0   1824\n",
       "149700  yes_0   1825\n",
       "149701  yes_0   1826\n",
       "149702  yes_0   1827\n",
       "149703  yes_0   1828\n",
       "\n",
       "[149704 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def add_missing(split):\n",
    "#     to_delete = ['go_1', 'finish_1' , 'later_0', 'all_0' ,'right_0', 'school_0', 'son_0','cow_1','yes_0']\n",
    "#     h5_path = os.path.join('np_arrays','dataset_standardize(refine2)_boarder_rotation_20','h5',split)\n",
    "#     df_origin = pd.read_csv(os.path.join(h5_path,'meta.csv'),index_col=0)\n",
    "#     df_origin = df_origin[~df_origin['Gloss'].isin(to_delete)]\n",
    "#     df_append = pd.read_csv(os.path.join(h5_path,'meta_append2.csv'),index_col=0)\n",
    "#     df_result = pd.concat([df_origin,df_append])\n",
    "#     df_result.reset_index(inplace=True)\n",
    "#     df_result.drop(columns=['index'],inplace=True)\n",
    "#     df_result.to_csv(os.path.join(h5_path,'meta.csv'))\n",
    "                     \n",
    "#     display(df_result)\n",
    "    \n",
    "# add_missing('train_0')\n",
    "# add_missing('val_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5d2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85466599999998,
   "position": {
    "height": "40px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
