{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c1620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\homan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from mediapipe.framework.formats.landmark_pb2 import NormalizedLandmark\n",
    "empty_pivot = NormalizedLandmark()\n",
    "empty_pivot.x = 0 \n",
    "empty_pivot.y = 0 \n",
    "empty_pivot.z = 0 \n",
    "empty_pivot.visibility = 0 \n",
    "\n",
    "def distance(pt1, pt2):\n",
    "    return ((pt1.x-pt2.x)**2 + (pt1.y-pt2.y)**2 + (pt1.z-pt2.z)**2)**0.5\n",
    "\n",
    "# Return: status, Landmarks, left vaild landmark, right valid landmark\n",
    "# Valid landmark: landmarks that are detected, else empty list\n",
    "# status: -1: not detected, 0: two hands, 1: left hand only, 2: right hand only\n",
    "# TODO: Handle Rotation/ Flip values here later\n",
    "def get_flatten_landmarks(image, holistic, use_visibility=False, scale=False):\n",
    "    if image is None:\n",
    "        return -1, [None]\n",
    "    if(image.shape[0]==0 or image.shape[1]==0):\n",
    "        return -1, [None]\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    status = -1\n",
    "\n",
    "    # Extract landmarks\n",
    "    pose = results.pose_landmarks\n",
    "    left_hand = results.left_hand_landmarks\n",
    "    right_hand = results.right_hand_landmarks\n",
    "\n",
    "    # Counting this to prevent the image show a tiny part of hands (20 is the max)\n",
    "    left_boundary_count = 0\n",
    "    right_boundary_count = 0\n",
    "\n",
    "    if pose is not None:\n",
    "        if scale:\n",
    "            pivot = pose.landmark[0]\n",
    "            a, b = pose.landmark[11], pose.landmark[12]\n",
    "            scale_length = distance(a,b)\n",
    "        else:\n",
    "            pivot = dataset.empty_pivot\n",
    "            scale_length = 1\n",
    "\n",
    "        pose_row = None\n",
    "        for i, landmark in enumerate(pose.landmark):\n",
    "#                 if i not in dataset.keep_pose:\n",
    "#                     continue # Skip Non-informative Landmarks\n",
    "            landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "            landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "            landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "            visibility = landmark.visibility\n",
    "\n",
    "            pose_row = landmark_x if pose_row is None else np.append(pose_row, landmark_x)\n",
    "            pose_row = np.append(pose_row, landmark_y)\n",
    "            pose_row = np.append(pose_row, landmark_z)\n",
    "            if use_visibility:\n",
    "                pose_row = np.append(pose_row, visibility)\n",
    "    else:\n",
    "        status = -1\n",
    "        # Invalid Image\n",
    "        return status, [None]\n",
    "\n",
    "    if left_hand is not None:\n",
    "        if scale:\n",
    "            pivot_left = left_hand.landmark[0]\n",
    "            a, b = left_hand.landmark[5], left_hand.landmark[0]\n",
    "            scale_length_left = distance(a,b)\n",
    "\n",
    "        left_row = None\n",
    "        for landmark in left_hand.landmark:\n",
    "            landmark_x_relative_hand = (landmark.x-pivot_left.x)/scale_length_left\n",
    "            landmark_y_relative_hand = (landmark.y-pivot_left.y)/scale_length_left\n",
    "            landmark_z_relative_hand = (landmark.z-pivot_left.z)/scale_length_left\n",
    "\n",
    "            landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "            landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "            landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "\n",
    "            left_row = landmark_x if left_row is None else np.append(left_row, landmark_x)\n",
    "            left_row = np.append(left_row, landmark_y)\n",
    "            left_row = np.append(left_row, landmark_z)\n",
    "\n",
    "            left_row = np.append(left_row, landmark_x_relative_hand)\n",
    "            left_row = np.append(left_row, landmark_y_relative_hand)\n",
    "            left_row = np.append(left_row, landmark_z_relative_hand)\n",
    "\n",
    "            if (not (landmark.x > 0  and landmark.x < 1)) or not (landmark.y > 0  and landmark.y < 1):\n",
    "                left_boundary_count+=1\n",
    "\n",
    "    if right_hand is not None:\n",
    "        if scale:\n",
    "            pivot_right = right_hand.landmark[0]\n",
    "            a, b = right_hand.landmark[5], right_hand.landmark[0]\n",
    "            scale_length_right = distance(a,b)\n",
    "\n",
    "        right_row = None\n",
    "        for landmark in right_hand.landmark:\n",
    "            landmark_x_relative_hand = (landmark.x-pivot_right.x)/scale_length_right\n",
    "            landmark_y_relative_hand = (landmark.y-pivot_right.y)/scale_length_right\n",
    "            landmark_z_relative_hand = (landmark.z-pivot_right.z)/scale_length_right\n",
    "\n",
    "            landmark_x = (landmark.x-pivot.x)/scale_length\n",
    "            landmark_y = (landmark.y-pivot.y)/scale_length\n",
    "            landmark_z = (landmark.z-pivot.z)/scale_length\n",
    "\n",
    "            right_row = landmark_x if right_row is None else np.append(right_row, landmark_x)\n",
    "            right_row = np.append(right_row, landmark_y)\n",
    "            right_row = np.append(right_row, landmark_z)\n",
    "\n",
    "            right_row = np.append(right_row, landmark_x_relative_hand)\n",
    "            right_row = np.append(right_row, landmark_y_relative_hand)\n",
    "            right_row = np.append(right_row, landmark_z_relative_hand)\n",
    "\n",
    "            if (not (landmark.x > 0  and landmark.x < 1)) or not (landmark.y > 0  and landmark.y < 1):\n",
    "                right_boundary_count+=1\n",
    "\n",
    "    # Two Hands Detected, but could be only showing a very tiny part\n",
    "    if left_hand is not None and right_hand is not None:\n",
    "        result = np.concatenate([pose_row,left_row,right_row])\n",
    "        result = result.reshape((1,result.shape[0]))\n",
    "        #print('both',result.shape)\n",
    "        status = 0\n",
    "        if left_boundary_count > 10: # Left Hand Invalid\n",
    "            status = 2\n",
    "        if right_boundary_count > 10: # Right Hand Invalid\n",
    "            status = 1\n",
    "        if left_boundary_count > 10 and right_boundary_count > 10: # Two Hands Are Invalid\n",
    "            status = -1\n",
    "    else:\n",
    "        # x->0.5 = middle, y->1 = bottom\n",
    "\n",
    "        #Single Hand Model\n",
    "        if left_hand is not None:\n",
    "            empty_arr = np.tile([0,0,0], 21*2)\n",
    "            result = np.concatenate([pose_row,left_row,empty_arr])\n",
    "            result = result.reshape((1,result.shape[0]))\n",
    "            #print('left',result.shape)\n",
    "            status = 1\n",
    "        elif right_hand is not None:\n",
    "            empty_arr = np.tile([0,0,0], 21*2)\n",
    "            result = np.concatenate([pose_row, empty_arr, right_row])\n",
    "            result = result.reshape((1,result.shape[0]))\n",
    "            #print('right',result.shape)\n",
    "            status = 2\n",
    "        else:\n",
    "            result = [None]\n",
    "            status = -1\n",
    "    return status, result\n",
    "\n",
    "        \n",
    "class frame_buffer:\n",
    "    import numpy as np\n",
    "    # Shape should be a 2d tuple\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.buffer = np.zeros(shape)\n",
    "        self.pointer = 0\n",
    "        self.item_count = 0\n",
    "    \n",
    "    def append(self, row):\n",
    "        #shape[0] : number of rows\n",
    "        self.buffer[self.pointer] = row\n",
    "        if self.item_count < self.shape[0]:\n",
    "            self.item_count += 1\n",
    "        if self.pointer < self.shape[0]-1:\n",
    "            self.pointer += 1\n",
    "        else:\n",
    "            # Reset counting from zero\n",
    "            self.pointer = 0\n",
    "            \n",
    "    def generate(self):\n",
    "        result = np.zeros(self.shape)\n",
    "        counter = 0\n",
    "        for i in range(self.pointer, self.shape[0]):\n",
    "            result[counter] = self.buffer[i]\n",
    "            counter += 1\n",
    "        for j in range(0, self.pointer):\n",
    "            result[counter] = self.buffer[j]\n",
    "            counter += 1\n",
    "        result = result.reshape(1,self.shape[0],self.shape[1])\n",
    "        return result\n",
    "    \n",
    "    def isFill(self):\n",
    "        return self.item_count == self.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8699af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import traceback\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039f68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectModel(idx):\n",
    "    models = [\n",
    "        {'path':'models_to_test\\model_final.tflite', 'visibility':False,'scale':True, 'label':'models_to_test\\OnehotEncoder_final.pkl'}\n",
    "    ]\n",
    "    model = tf.lite.Interpreter(model_path=models[idx]['path'])\n",
    "    label = models[idx].get(\"label\")\n",
    "    return model, models[idx]['visibility'], models[idx]['scale'], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187cbbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\homan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, visibility, scale, label= selectModel(0)\n",
    "model.allocate_tensors()\n",
    "input_details = model.get_input_details()\n",
    "output_details = model.get_output_details()\n",
    "\n",
    "if label is None:\n",
    "    file = open(f'models_to_test\\OnehotEncoder.pkl','rb')\n",
    "    encoder = pickle.load(file)\n",
    "else:\n",
    "    file = open(label,'rb')\n",
    "    encoder = pickle.load(file)\n",
    "    \n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "\n",
    "# Test Model\n",
    "#cap = cv2.VideoCapture('videos_reconstruct/test/family/0/20988.mp4')\n",
    "cap = cv2.VideoCapture(0)\n",
    "#set the width and height, and UNSUCCESSFULLY set the exposure time\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 720)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1280)\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "    buffer = frame_buffer(shape=(20,351))\n",
    "        \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        #frame = frame[:,50:frame.shape[0]-50]\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = image[:,100:-100]\n",
    "        image = image.copy()\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Flip Output\n",
    "        frame = cv2.flip(frame,1)\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Concate rows\n",
    "        row = get_flatten_landmarks(image, holistic, visibility, scale)\n",
    "        #print(row[1])\n",
    "#         if row[0] != -1:\n",
    "#             print(row[1].flatten())\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            \n",
    "            # 1. Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                     mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "\n",
    "            # 2. Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                     mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "\n",
    "            # 3. Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                     mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                     mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                     )\n",
    "            \n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "\n",
    "\n",
    "            # Get status box\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "\n",
    "            \n",
    "            if row[0] != -1:\n",
    "                buffer.append(row[1])\n",
    "                if buffer.isFill():\n",
    "                    to_predict = buffer.generate()\n",
    "                    to_predict = np.float32(to_predict)\n",
    "                    model.set_tensor(input_details[0]['index'], to_predict)\n",
    "                    model.invoke()\n",
    "                    predict_result = model.get_tensor(output_details[0]['index'])\n",
    "                    \n",
    "                    predict_label = np.argmax(predict_result, axis=None, out=None)\n",
    "                    body_language_prob = predict_result[0, predict_label]\n",
    "                    body_language_class = encoder.inverse_transform(predict_result)[0]\n",
    "                    \n",
    "                    cv2.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "                    \n",
    "                    # Display Class\n",
    "                    cv2.putText(image, 'CLASS'\n",
    "                                , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, str(body_language_class)\n",
    "                                , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                    cv2.putText(image, str(body_language_class), coords, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Display Probability\n",
    "                    cv2.putText(image, 'PROB'\n",
    "                                , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, str(round(body_language_prob,2))\n",
    "                                 , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # print(traceback.format_exc())\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1fe1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.txt', 'w') as f:\n",
    "    for i in range(84):\n",
    "        onehot_empty = np.zeros((1,84))\n",
    "        onehot_empty[0][i] = 1\n",
    "        k = encoder.inverse_transform(onehot_empty)[0][0]\n",
    "        for i in range(0,10):\n",
    "            k = k.replace(f\"_{i}\",\"\")\n",
    "        f.write(\"%s\\n\" % k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73d6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
